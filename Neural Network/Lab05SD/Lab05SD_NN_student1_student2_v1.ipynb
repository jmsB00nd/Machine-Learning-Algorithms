{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID Lab05. Neural Networks\n",
    "\n",
    "<p style='text-align: right;font-style: italic;'>Designed by: Mr. Abdelkrime Aries</p>\n",
    "\n",
    "In this lab, we will learn about neural networks (NN).\n",
    "We will implement backpropagation to understand how a NN works.\n",
    "Then, we will test many concepts:\n",
    "- Parameters' initialization\n",
    "- Activation functions effect\n",
    "- Optimization functions for NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "- **Member 01**: Arab Hamza\n",
    "- **Member 02**: Boukoufallah Abdallah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.3', '2.2.3', '3.9.2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib      import colors \n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow              import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type, Union\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Algorithms implementation\n",
    "\n",
    "In this section, we will try to implement a NN.\n",
    "A network is composed of many layers, each of many neurons.\n",
    "\n",
    "\n",
    "**>> Try to use \"numpy\" which will save a lot of time and effort** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This class is abstract; implement it\n",
      "This class is abstract; implement it\n",
      "works3\n"
     ]
    }
   ],
   "source": [
    "# API (Do not change, or you'll see double)\n",
    "# This is just an interface to define Activation and Loss functions' behaviour\n",
    "\n",
    "# =====================================================\n",
    "# =========== Activation function API =================\n",
    "# =====================================================\n",
    "class Activation(object): \n",
    "    \"\"\"Base Class to implement activation functions\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        raise Exception('This class is abstract; implement it')\n",
    "      \n",
    "    def activate(self, Z):\n",
    "        \"\"\"Calculates the activation based on Z\"\"\"\n",
    "        raise Exception('Must be implemented in child class')\n",
    "\n",
    "    def partial(self, Z, H):\n",
    "        \"\"\"Calculates partial derivative based on Z and the activation H\"\"\"\n",
    "        raise Exception('Must be implemented in child class')\n",
    "\n",
    "# =====================================================\n",
    "# ============== Loss function API ====================\n",
    "# =====================================================\n",
    "class Loss(object): \n",
    "    \"\"\"Base Class to implement loss functions\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        raise Exception('This class is abstract; implement it')\n",
    "\n",
    "    def calculate(self, H, Y):\n",
    "        \"\"\"Calculates the loss based on the infered output H and the real output Y\"\"\"\n",
    "        raise Exception('Must be implemented in child class')\n",
    "    \n",
    "    def partial(self, H, Y):\n",
    "        \"\"\"Calculates partial derivative based on the infered output H and the real output Y\"\"\"\n",
    "        raise Exception('Must be implemented in child class')\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# This class is abstract; implement it\n",
    "# This class is abstract; implement it\n",
    "# works3\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    aa = Activation()\n",
    "    print('works1')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    ll = Loss()\n",
    "    print('works2')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "class ActivationChild(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "try:\n",
    "    aa = ActivationChild()\n",
    "    print('works3')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Activation functions\n",
    "\n",
    "You have to implement both logistic regression and ReLU activation functions. \n",
    "\n",
    "Logistic regression:\n",
    "$$H = \\sigma(Z) = \\frac{1}{1+e^{-Z}} \\ \\ \\ \\ \\ \\ \\frac{\\partial \\sigma(Z)}{\\partial Z} = \\sigma(Z) (1-\\sigma(Z))$$\n",
    "\n",
    "ReLU:\n",
    "$$H = ReLU(Z) = \\begin{cases} Z & \\text{if } Z > 0 \\\\ 0 & \\text{otherwise}\\end{cases}\n",
    "\\ \\ \\ \\ \\ \\ \n",
    "\\frac{\\partial ReLU(Z)}{\\partial Z} = \\begin{cases} 1 & \\text{if } Z > 0 \\\\ 0 & \\text{otherwise}\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Logistic Activation class\n",
    "class Logistic(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def activate(self, Z):\n",
    "        return 1 / (1+np.exp(-Z))\n",
    "    \n",
    "    def partial(self, Z, H):\n",
    "        return H*(1-H)\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "lg = Logistic()\n",
    "z4_1       = np.array([1.666, 1.68])\n",
    "a4_1       = lg.activate(z4_1)\n",
    "a4_1p      = lg.partial(z4_1, a4_1)\n",
    "\n",
    "a4_1, a4_1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 2., 0.]), array([0., 1., 0.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: ReLU Activation class\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def activate(self, Z):\n",
    "        return np.array([float(max(i,0)) for i in Z])\n",
    "    \n",
    "    def partial(self, Z, H):\n",
    "        return np.array([float(1) if i > 0 else float(0) for i in H])\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([0., 2., 0.]), array([0., 1., 0.]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "relu = ReLU()\n",
    "z_t  = np.array([0, 2, -3])\n",
    "a_t  = relu.activate(z_t)\n",
    "p_t  = relu.partial(z_t, a_t)\n",
    "\n",
    "a_t, p_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Loss functions\n",
    "\n",
    "Binary Cross ntropy (BCE) loss function is calculated as:\n",
    "$$BCE(Y, H) = - ( Y \\log(H) + (1-Y) \\log(1-H))$$\n",
    "\n",
    "It's derivative is calculated as:\n",
    "$$\\frac{\\partial BCE}{\\partial H} = \\frac{H-Y}{H - H^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: BCE Loss class\n",
    "class BCE(Loss):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calculate(self, H, Y):\n",
    "        return -(Y*np.log(H) + (1-Y)*np.log(1-H))\n",
    "    \n",
    "    def partial(self, H, Y):\n",
    "        return (H-Y)/(H-H**2)\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "bce = BCE()\n",
    "\n",
    "H = np.array([0.840 , 0.843])\n",
    "Y = np.array([0., 1.])\n",
    "J = bce.calculate(H, Y)\n",
    "DJ = bce.partial(H, Y)\n",
    "\n",
    "J, DJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Neuron\n",
    "\n",
    "Let us implement retropropagation of one neuron.\n",
    "\n",
    "$$\\delta^{(l)} = \\frac{\\partial f^{(l)}}{\\partial z^{(l)}} w^{(l+1)} \\delta^{(l+1)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{(l)}} = a^{(l-1)} \\delta^{(l)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b^{(l)}} = \\delta^{(l)}$$\n",
    "\n",
    "The product is a matrix product (on $M$) and we must take the average of the outputs (on $Ln$).\n",
    "\n",
    "The function that updates the parameters takes as input:\n",
    "- $W[L_p]$ a list of weights; a vector of size $L_p$ (the number of neurons in the previous layer)\n",
    "- $b$ bias\n",
    "- $Z[M]$ the linear combination of the current neuron; a vector of size $M$ (the number of samples)\n",
    "- $A[M]$ the activation of the current neuron; a vector of size $M$\n",
    "- $A\\_past[M, L_p]$ the activations of the neurons of the previous layer; a matrix of size is $(M * L_p)$\n",
    "- $Delta\\_next[M, Ln]$ the delta calculated in the next layer; a matrix of size $M * L_n$ ($L_n$: the number of neurons in the next layer)\n",
    "- $W\\_next[Ln]$ the weights to the next layer; a vector of size $Ln$\n",
    "- $act$ is an \"Activation\" type object; it provides two methods: \"act.activate\" and \"act.partial\"\n",
    "- $alpha$ the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.49375218, 0.2046736 ]),\n",
       " -0.30324311474187016,\n",
       " array([ 0.00696306, -0.00047683]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Neuron Backpropagation\n",
    "def backpropagate_neuron(W, b, Z, A, A_past, Delta_next, W_next, act, alpha=1.):\n",
    "    \n",
    "    dz = act.partial(Z, A)\n",
    "\n",
    "    Delta = dz * np.dot(Delta_next, W_next)\n",
    "    \n",
    "    dW = np.dot(A_past.T, Delta) / A_past.shape[0]\n",
    "    db = np.mean(Delta)\n",
    "    \n",
    "    Wn = W - alpha * dW\n",
    "    bn = b - alpha * db\n",
    "    return Wn, float(bn), Delta\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST \n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([0.49375218, 0.2046736 ]),\n",
    "#  -0.30324311474187016,\n",
    "#  array([ 0.00696306, -0.00047683]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "W_t = np.array([0.5, 0.2])\n",
    "b_t = -0.3\n",
    "Z_t = np.array([0.5, 2.2])\n",
    "\n",
    "# A_t[M] (This neuron's current activation)\n",
    "A_t = np.array([0.62245933, 0.90024951])\n",
    "\n",
    "# A_past_t[M, L_p] (This neuron's past layer's activations)\n",
    "A_past_t = np.array([[2., -1.], [3., 5.]])\n",
    "\n",
    "# Delta_next_t[M, L_n] (This neuron's next layer's deltas)\n",
    "Delta_next_t = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "\n",
    "# W_next_t[L_n] (This neuron's next layer's Weights)\n",
    "W_next_t = np.array([0.3, -0.1])\n",
    "\n",
    "act = Logistic() # Activation function\n",
    "\n",
    "W_nouv, b_nouv, Delta_nouv = backpropagate_neuron(W_t, b_t, Z_t, A_t, A_past_t, Delta_next_t, W_next_t, act, alpha=1.)\n",
    "\n",
    "W_nouv, b_nouv, Delta_nouv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2_1 = [0.5 2.2]\n",
      "a2_1 = [0.62245933 0.90024951]\n",
      "partial(a2_1) = [0.23500371 0.08980033]\n",
      "past b = -0.3\n",
      "past w = [0.5 0.2]\n",
      "delta2 = [ 0.00696306 -0.00047683]\n",
      "new b = -0.30324311473938026\n",
      "new w = [0.49375218 0.2046736 ]\n"
     ]
    }
   ],
   "source": [
    "# Nothing to do here \n",
    "class Neuron(object):\n",
    "    def __init__(self, in_size, activation=Logistic()):\n",
    "        self.b   = 0.\n",
    "        self.w   = np.array([0.] * in_size)\n",
    "        self.act = activation\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.w = np.random.rand(len(self.w))\n",
    "        self.b = np.random.rand(1)[0]\n",
    "        \n",
    "    def __aggregate(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activate(self, X):\n",
    "        self.a_past = X\n",
    "        self.z      = self.__aggregate(X)\n",
    "        self.a      = self.act.activate(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def update(self, delta_next, w_next, alpha=1.):\n",
    "        w_past                = self.w.copy()\n",
    "        self.w, self.b, delta = backpropagate_neuron(self.w, self.b, self.z, self.a, self.a_past, delta_next, w_next, self.act, alpha=alpha)\n",
    "        return delta, w_past\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# z2_1 = [0.5 2.2]\n",
    "# a2_1 = [0.62245933 0.90024951]\n",
    "# partial(a2_1) = [0.23500371 0.08980033]\n",
    "# past b = -0.3\n",
    "# past w = [0.5 0.2]\n",
    "# delta2 = [ 0.00696306 -0.00047683]\n",
    "# new b = -0.30324311473938026\n",
    "# new w = [0.49375218 0.2046736 ]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# New neuron with two inputs\n",
    "n = Neuron(2)\n",
    "# ---------------------\n",
    "# We should not affect the weights directly\n",
    "# Here, it's just to have the same weights of the output neuron in the example seen in the lecture\n",
    "# We will reproduce the parameters of neuron 1 hidden layer 1 (layer 2)\n",
    "n.b = -0.3\n",
    "n.w = np.array([0.5, 0.2])\n",
    "# ---------------------\n",
    "\n",
    "# M X Lp (Here A1 = X; the input)\n",
    "A1 = np.array([[2., -1.], [3., 5.]])\n",
    "# M X Ln (Delta of the next layer)\n",
    "Delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "W3_1 = np.array([0.3, -0.1])\n",
    "\n",
    "\n",
    "A2_1 = n.activate(A1)\n",
    "print(\"z2_1 = \" + str(n.z))\n",
    "print(\"a2_1 = \" + str(A2_1))\n",
    "# The partial derivative of logistic function does not need Z, so we pass 0 (to not calculte it)\n",
    "print(\"partial(a2_1) = \" + str(n.act.partial(0, A2_1)))\n",
    "print(\"past b = \" + str(n.b))\n",
    "\n",
    "Delta2, W2_past = n.update(Delta3, W3_1) \n",
    "\n",
    "print(\"past w = \" + str(W2_past))\n",
    "print(\"delta2 = \" + str(Delta2))\n",
    "print(\"new b = \" + str(n.b))\n",
    "print(\"new w = \" + str(n.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Layer\n",
    "\n",
    "A layer is a list of neurons.\n",
    "In this case, the packpropagation of the layer takes:\n",
    "- **neurons**: a list of activated neurons.\n",
    "- **Delta_next**: Delta from the next layer.\n",
    "- **W_next**: next layer's weights\n",
    "- **alpha**: learning rate\n",
    "\n",
    "The function returns a tuple:\n",
    "- Deltas of the current layer (all neurons)\n",
    "- Past weights of the current layer (all neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deltas : [[ 0.00696306  0.00682726]\n",
      " [-0.00047683 -0.00017109]]\n",
      "past W : [[ 0.00696306  0.00682726]\n",
      " [-0.00047683 -0.00017109]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Layer Backpropagation\n",
    "def backpropagate_layer(neurons, Delta_next, W_next, alpha=1.):\n",
    "    W_pasts = []\n",
    "    Deltas  = []\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        delta, w_past = neuron.update(Delta_next, W_next[i], alpha=alpha)\n",
    "        Deltas.append(delta)\n",
    "        W_pasts.append(w_past)\n",
    "    return np.array(Deltas).T, np.array(W_pasts).T\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# deltas : [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]\n",
    "# past W : [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "neurons_t = [Neuron(2), Neuron(2)]\n",
    "\n",
    "# We should not directly assign  weights \n",
    "# Here, it is just to have the same weights of the output neuron of the lecture's example\n",
    "neurons_t[0].b = -0.3\n",
    "neurons_t[0].w = np.array([0.5, 0.2])\n",
    "neurons_t[1].b = 0.5\n",
    "neurons_t[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3 = np.array([[0.3, -0.1],[0.5, -0.3]])\n",
    "\n",
    "a1 = np.array([[2., -1.], [3., 5.]])\n",
    "neurons_t[0].activate(a1)\n",
    "neurons_t[1].activate(a1)\n",
    "\n",
    "Deltas2, W_pasts2 = backpropagate_layer(neurons_t, delta3, w3)\n",
    "\n",
    "print(\"deltas : \" + str(Deltas2))\n",
    "print(\"past W : \" + str(Deltas2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: [[0.62245933 0.66818777]\n",
      " [0.90024951 0.96770454]]\n",
      "Deltas: [[ 0.00696306  0.00682726]\n",
      " [-0.00047683 -0.00017109]]\n"
     ]
    }
   ],
   "source": [
    "# Do not edit\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, size, in_size, activation=Logistic()):\n",
    "        self.neurons = [Neuron(in_size, activation=activation) for i in range(size)]\n",
    "        \n",
    "    def randomize(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.randomize()\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = []\n",
    "        for neuron in self.neurons:\n",
    "            activations.append(neuron.activate(X))\n",
    "        return np.array(activations).T\n",
    "    \n",
    "    def backward(self, Delta_next, W_next, alpha=1.):\n",
    "        return backpropagate_layer(self.neurons, Delta_next, W_next, alpha=alpha)\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# Activations: [[0.62245933 0.66818777]\n",
    "#  [0.90024951 0.96770454]]\n",
    "# Deltas: [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# layer 2 (2 neurons, 2 inputs)\n",
    "c2 = Layer(2, 2)\n",
    "\n",
    "# We should not directly assign  weights \n",
    "# Here, it is just to have the same weights of the output neuron of the lecture's example\n",
    "c2.neurons[0].b = -0.3\n",
    "c2.neurons[0].w = np.array([0.5, 0.2])\n",
    "c2.neurons[1].b = 0.5\n",
    "c2.neurons[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3 = np.array([[0.3, -0.1],[0.5, -0.3]])\n",
    "\n",
    "a1 = np.array([[2., -1.], [3., 5.]])\n",
    "a2 = c2.forward(a1)\n",
    "print(\"Activations: \" + str(a2))\n",
    "\n",
    "Deltas2, W_pasts2 = c2.backward(delta3, w3)\n",
    "\n",
    "print(\"Deltas: \" + str(Deltas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Network\n",
    "\n",
    "A network is composed of a list of layers.\n",
    "Here, we want to implement backpropagation over layers (from the last one into the first)\n",
    "\n",
    "\n",
    "For the last layer:\n",
    "- w_past is a list of ones of the this layer's size. Such as [[1.], [1.], ...]\n",
    "- delta_past is the gradient of the loss function \"J_prime\" which must be calculated using the cost function, the output H and the real output Y\n",
    "\n",
    "Then, each layer apply a backpropagation and returns delta_past, w_past which will be fed into the previous layer.\n",
    "\n",
    "The function must return the cost J which is the average of all losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w4_1 = [0.51494626 0.56592079]\n",
      "w3_1 = [0.2665629 0.4641237]\n",
      "w3_2 = [-0.13199638 -0.33433028]\n",
      "w2_1 = [0.49375219 0.2046736 ]\n",
      "w2_2 = [0.29342937 0.40384135]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0020916974430962)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Network Backpropagation\n",
    "def backpropagate_network(layers, cost, H, Y):\n",
    "    J = np.mean(cost.calculate(H, Y))\n",
    "\n",
    "    delta_past = cost.partial(H, Y)\n",
    "    w_past = np.ones((delta_past.shape[0], 1)) \n",
    "    for layer in reversed(layers):\n",
    "        delta_past, w_past = backpropagate_layer(layer.neurons, delta_past, w_past)\n",
    "\n",
    "    return J\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# w4_1 = [0.51494626 0.56592079]\n",
    "# w3_1 = [0.2665629 0.4641237]\n",
    "# w3_2 = [-0.13199638 -0.33433028]\n",
    "# w2_1 = [0.49375219 0.2046736 ]\n",
    "# w2_2 = [0.29342937 0.40384135]\n",
    "# 1.0020916974430965\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X = np.array([[2., -1.], [3., 5.]])\n",
    "Y = np.array([[0.], [1.]])\n",
    "\n",
    "layers = [ Layer(2, 2), Layer(2, 2), Layer(1, 2) ]\n",
    "\n",
    "# We should not directly assign  weights \n",
    "# Here, it is just to have the same weights of the output neuron of the lecture's example\n",
    "layers[0].neurons[0].b = -0.3\n",
    "layers[0].neurons[0].w = np.array([0.5, 0.2])\n",
    "layers[0].neurons[1].b = 0.5\n",
    "layers[0].neurons[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "layers[1].neurons[0].b = -0.3\n",
    "layers[1].neurons[0].w = np.array([0.3, 0.5])\n",
    "layers[1].neurons[1].b = -0.2\n",
    "layers[1].neurons[1].w = np.array([-0.1, -0.3])\n",
    "\n",
    "layers[2].neurons[0].b = 1.\n",
    "layers[2].neurons[0].w = np.array([0.7, 0.7])\n",
    "\n",
    "H = X\n",
    "for layer in layers:\n",
    "    H = layer.forward(H)\n",
    "\n",
    "J = backpropagate_network(layers, bce, H, Y)\n",
    "\n",
    "print(\"w4_1 = \" + str(layers[2].neurons[0].w))\n",
    "print(\"w3_1 = \" + str(layers[1].neurons[0].w))\n",
    "print(\"w3_2 = \" + str(layers[1].neurons[1].w))\n",
    "print(\"w2_1 = \" + str(layers[0].neurons[0].w))\n",
    "print(\"w2_2 = \" + str(layers[0].neurons[1].w))\n",
    "\n",
    "J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.0020916974430962\n",
      "w4_1 = [0.51494626 0.56592079]\n",
      "w3_1 = [0.2665629 0.4641237]\n",
      "w3_2 = [-0.13199638 -0.33433028]\n",
      "w2_1 = [0.49375219 0.2046736 ]\n",
      "w2_2 = [0.29342937 0.40384135]\n",
      "Prediction: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Do not change\n",
    "class NN(object):\n",
    "    def __init__(self, in_size, cost=bce, alpha=1.):\n",
    "        self.current_size = in_size # the last layer's size \n",
    "        self.cost = cost\n",
    "        self.alpha = alpha\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, size, activation=Logistic()):\n",
    "        new_layer = Layer(size, self.current_size, activation=activation)\n",
    "        self.layers.append(new_layer)\n",
    "        self.current_size = size\n",
    "        \n",
    "    def randomize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize()\n",
    "    \n",
    "    def predict(self, X): \n",
    "        Y = X\n",
    "        if self.norm:\n",
    "            Y = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            Y = layer.forward(Y)\n",
    "        if Y.ndim == 2 and Y.shape[1] == 1:\n",
    "            Y = Y.flatten()\n",
    "        return np.where(Y < 0.5, 0, 1)\n",
    "    \n",
    "    \n",
    "    def _one_iteration(self, X, Y):\n",
    "        # forward propagation\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "            \n",
    "        # cost and its derivative calculation\n",
    "        YY = np.array(Y)\n",
    "        if YY.ndim < 2 : \n",
    "            YY = YY[:, np.newaxis]\n",
    "        \n",
    "        # backward propagation \n",
    "        J = backpropagate_network(self.layers, self.cost, a, YY)\n",
    "    \n",
    "        return J\n",
    "    \n",
    "    def fit(self, X, Y, nbr_it=100, norm=False):\n",
    "        costs = []\n",
    "        X_norm = X\n",
    "        self.norm = norm\n",
    "        if norm:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            X_norm = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "\n",
    "        for i in range(nbr_it): \n",
    "            J = self._one_iteration(X_norm, Y)\n",
    "            costs.append(J)\n",
    "        return costs\n",
    "    \n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# cost = 1.0020916974430965\n",
    "# w4_1 = [0.51494626 0.56592079]\n",
    "# w3_1 = [0.2665629 0.4641237]\n",
    "# w3_2 = [-0.13199638 -0.33433028]\n",
    "# w2_1 = [0.49375219 0.2046736 ]\n",
    "# w2_2 = [0.29342937 0.40384135]\n",
    "# Prediction: [0 1]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X = np.array([[2., -1.], [3., 5.]])\n",
    "Y = np.array([0., 1.])\n",
    "\n",
    "nn = NN(2) # 2 features\n",
    "nn.add_layer(2) # add a hidden layer with 2 neurons\n",
    "nn.add_layer(2) # add a hidden layer with 2 neurons\n",
    "nn.add_layer(1) # add an output layer with 1 neuron\n",
    "\n",
    "# We should not affect the weights directly\n",
    "# Here, it's just to have the same weights of the output neuron in the example seen in the lecture\n",
    "nn.layers[0].neurons[0].b = -0.3\n",
    "nn.layers[0].neurons[0].w = np.array([0.5, 0.2])\n",
    "nn.layers[0].neurons[1].b = 0.5\n",
    "nn.layers[0].neurons[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "nn.layers[1].neurons[0].b = -0.3\n",
    "nn.layers[1].neurons[0].w = np.array([0.3, 0.5])\n",
    "nn.layers[1].neurons[1].b = -0.2\n",
    "nn.layers[1].neurons[1].w = np.array([-0.1, -0.3])\n",
    "\n",
    "nn.layers[2].neurons[0].b = 1.\n",
    "nn.layers[2].neurons[0].w = np.array([0.7, 0.7])\n",
    "\n",
    "J = nn._one_iteration(X, Y)\n",
    "\n",
    "print(\"cost = \" + str(J))\n",
    "print(\"w4_1 = \" + str(nn.layers[2].neurons[0].w))\n",
    "print(\"w3_1 = \" + str(nn.layers[1].neurons[0].w))\n",
    "print(\"w3_2 = \" + str(nn.layers[1].neurons[1].w))\n",
    "print(\"w2_1 = \" + str(nn.layers[0].neurons[0].w))\n",
    "print(\"w2_2 = \" + str(nn.layers[0].neurons[1].w))\n",
    "\n",
    "nn.fit(X, Y, nbr_it=200)\n",
    "print(\"Prediction: \" + str(nn.predict(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
